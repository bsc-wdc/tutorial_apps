{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dislib hands-on exercise\n",
    "\n",
    "This notebook includes some exercises to learn the basics of using [dislib](https://dislib.bsc.es).\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Apart from dislib, this notebook requires [PyCOMPSs 2.8 or higher](https://www.bsc.es/research-and-development/software-and-apps/software-list/comp-superscalar/).\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "\n",
    "First, we need to start an interactive PyCOMPSs session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycompss.interactive as ipycompss\n",
    "import os\n",
    "\n",
    "os.environ[\"ComputingUnits\"] = \"1\"\n",
    "\n",
    "if 'BINDER_SERVICE_HOST' in os.environ:\n",
    "    ipycompss.start(graph=True,\n",
    "                    project_xml='../xml/project.xml',\n",
    "                    resources_xml='../xml/resources.xml')\n",
    "else:\n",
    "    ipycompss.start(graph=True, monitor=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import dislib and we are all set to start working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dislib as ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning with dislib\n",
    "\n",
    "Dislib provides an estimator-based API very similar to [scikit-learn](https://scikit-learn.org/stable/). An estimator is anything that learns from data. To illustrate how an estimator works, let's first generate some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "x_np, y = make_blobs(n_samples=1500, random_state=170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x_np` and `y` are random samples and labels. Samples are vectors and labels are numbers that represent the category of each sample. In this example, we are going to run clustering algorithms, which are useful to understand **unlabeled** data, and thus, we will not use `y`. \n",
    "\n",
    "Since the samples in `x_np` are 2-dimensional, we can plot them and see that there are 3 clusters in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(x_np[:, 0], x_np[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use dislib, we first need to convert `x` to a ds-array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ds.array(x_np, block_size=(300, 2))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using DBSCAN\n",
    "\n",
    "We have seen before the behaviour of K-means. K-means is a simple yet effective clustering method. However, K-means has a major drawback: the number of clusters needs to be defined beforehand. This is no always possible, and other clustering methods have attempted to address this limitation.\n",
    "\n",
    "An example is DBSCAN, which is a density based clustering algorithm. In DBSCAN, users define density using two parameters: `eps` and `min_samples`. The algorithm then finds an arbitrary number of clusters based on these two parameters.\n",
    "\n",
    "Your task now is to experiment with different `eps` and `min_samples` values to see how DBSCAN performs with the blob data!\n",
    "\n",
    "**WARNING:** DBSCAN works a bit different to K-means. You can find its API reference [here](https://dislib.readthedocs.io/en/stable/dislib.cluster.dbscan.html#dislib.cluster.dbscan.base.DBSCAN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dislib.cluster import DBSCAN\n",
    "\n",
    "# fill in the values for eps and min_samples\n",
    "dbscan = DBSCAN(eps=1, min_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and predict the labels for x\n",
    "y_pred = dbscan.fit_predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, you can plot the results with the following code (assuming predicted labels are in `y_pred`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the color of each sample to the predicted label\n",
    "plt.scatter(x_np[:, 0], x_np[:, 1], c=y_pred.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try with different data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "x_np, _ = make_circles(n_samples=1500, factor=.5, noise=.05, random_state=170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use K-means and DBSCAN to cluster the data in x_np. Which algorithm performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ds-array\n",
    "x = ds.array(x_np, block_size=(300, 2))\n",
    "\n",
    "from dislib.cluster import KMeans\n",
    "\n",
    "# Create estimators\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "dbscan = DBSCAN(eps=0.1, min_samples=5)\n",
    "\n",
    "# Fit and predict labels\n",
    "y_dbscan = dbscan.fit_predict(x)\n",
    "y_km = kmeans.fit_predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to plot the results\n",
    "\n",
    "ax = plt.subplot(121)\n",
    "ax.title.set_text(\"DBSCAN\")\n",
    "ax.scatter(x_np[:, 0], x_np[:, 1], c=y_dbscan.collect())\n",
    "ax = plt.subplot(122)\n",
    "ax.title.set_text(\"K-means\")\n",
    "ax.scatter(x_np[:, 0], x_np[:, 1], c=y_km.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Now we will solve an exercise using the digits data set from scikit-learn. Samples in this data set represent images of handwritten digits (0 to 9), where each feature represents a pixel in the image.\n",
    "\n",
    "First, we load the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "x_np = digits.data\n",
    "y = digits.target.reshape(-1, 1)\n",
    "\n",
    "x_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x_np` contains 1797 samples of 64 features, and `y` contains the labels, which in this case is the handwritten number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit = x_np[23]\n",
    "digit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see actually see the digit by reshaping the vector and plotting it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digit = digit.reshape((8,8))\n",
    "plt.imshow(digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and the corresponding label should be a 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the original data set has 10 different labels, we can simplify the problem by converting it into a binary classification problem, where odd numbers have label=0 and even numbers have label=1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y%2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `y[23]` should be 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is different from clustering in that labels are also used for the fitting process. Once a classifier is fitted, we can use it to label unlabeled data.\n",
    "\n",
    "To simulate having labeled (training) and unlabeled (test) data, we split the digits data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_np, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x_train` and `x_test` now contain 75% and 25% of the samples in `x` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to use `CascadeSVM` and `RandomForestClassifier` to classify the digits data, and get the accuracy obtained!\n",
    "\n",
    "### Hints:\n",
    "\n",
    "- You can find dislib's API reference [here](https://dislib.readthedocs.io/en/stable/api-reference.html).\n",
    "- Remember to convert data to ds-arrays before passing them to the classifiers.\n",
    "- Do not worry too much about the classifiers' parameters, you can use the default values.\n",
    "- Use the train data to fit the estimators, and the test data to check the accuracy.\n",
    "- Accuracy can be obtained using the `score` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping is needed because of reasons\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "# convert your data to ds-arrays\n",
    "x_ds_train = ds.array(x_train, block_size=(300, 64))\n",
    "y_ds_train = ds.array(y_train, block_size=(300, 1))\n",
    "\n",
    "x_ds_test = ds.array(x_test, (300, 64))\n",
    "y_ds_test = ds.array(y_test, (300, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dislib.classification import CascadeSVM\n",
    "\n",
    "# create CascadeSVM estimator\n",
    "csvm = CascadeSVM(gamma=0.1)\n",
    "\n",
    "\n",
    "# fit the estimator with training data\n",
    "csvm.fit(x_ds_train, y_ds_train)\n",
    "\n",
    "\n",
    "# print the accuracy on the test data\n",
    "score = csvm.score(x_ds_test, y_ds_test)\n",
    "\n",
    "from pycompss.api.api import compss_wait_on\n",
    "print(compss_wait_on(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do the same as above using the RandomForestClassifier :)\n",
    "\n",
    "from dislib.classification import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(x_ds_train, y_ds_train)\n",
    "\n",
    "print(compss_wait_on(rf.score(x_ds_test, y_ds_test)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which classifier gets better results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization\n",
    "\n",
    "Classifiers' performance is highly sensitive to the initialization parameters (or hyperparameters), and it is difficult to know which parameters are optimal beforehand. Thankfully, there are hyperparameter optimization techniques that allow us to find good parameters for our classification problem.\n",
    "\n",
    "One of these techniques is grid search with cross-validation. This model selection algorithm performs an exhaustive search on a predefined set of hyperparameters to find the optimal ones.\n",
    "\n",
    "Try to improve the score of the CascadeSVM classifier using grid search! You can find GridSearchCV reference [here](https://dislib.readthedocs.io/en/stable/dislib.model_selection.html#dislib.model_selection.GridSearchCV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dislib.model_selection import GridSearchCV\n",
    "\n",
    "# hyperparameter search space\n",
    "params = {\"gamma\" : (0.1, 0.01, 0.0001), \"c\" : (1, 10, 100)}\n",
    "\n",
    "csvm = CascadeSVM()\n",
    "\n",
    "# use grid search with your training data (it might take a while, be patient)\n",
    "searcher = GridSearchCV(csvm, params)\n",
    "searcher.fit(x_ds_train, y_ds_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to print the results in a nice way\n",
    "import pandas as pd\n",
    "pd.DataFrame(searcher.cv_results_)[[\"params\", \"mean_test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try the optimal parameters with the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimal parameters here\n",
    "csvm = CascadeSVM(gamma=0.0001, c=100)\n",
    "\n",
    "csvm.fit(x_ds_train, y_ds_train)\n",
    "\n",
    "from pycompss.api.api import compss_wait_on\n",
    "compss_wait_on(csvm.score(x_ds_test, y_ds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the results improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the session\n",
    "\n",
    "To finish the session, we need to stop PyCOMPSs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipycompss.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
